# -*- coding: utf-8 -*-
"""AI PoweChatBot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QiMqGqXuP8NdnphkV4d3x3Lt99vjulba
"""

import torch
import datetime
import json
import matplotlib.pyplot as plt
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    pipeline
)

print("Loading models... Please wait...\n")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# =====================
# Chat Model
# =====================

chat_model_name = "microsoft/DialoGPT-medium"
tokenizer = AutoTokenizer.from_pretrained(chat_model_name)
chat_model = AutoModelForCausalLM.from_pretrained(chat_model_name).to(device)

# =====================
# Sentiment
# =====================

sentiment_analyzer = pipeline("sentiment-analysis", device=0 if torch.cuda.is_available() else -1)

# =====================
# Summarizer (Manual - No pipeline task issues)
# =====================

summarizer_tokenizer = AutoTokenizer.from_pretrained("facebook/bart-large-cnn")
summarizer_model = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-large-cnn").to(device)

# =====================
# QA
# =====================

qa_pipeline = pipeline("question-answering", device=0 if torch.cuda.is_available() else -1)

print("All models loaded successfully!\n")

# =====================
# Functions
# =====================

def summarize_conversation(text):
    if len(text.split()) < 50:
        return text

    inputs = summarizer_tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=1024
    ).to(device)

    summary_ids = summarizer_model.generate(
        inputs["input_ids"],
        max_length=100,
        min_length=30,
        num_beams=4,
        early_stopping=True
    )

    return summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print("System ready ðŸš€")

# ==========================================
# IMPORTS
# ==========================================

import torch
import datetime
import json
import matplotlib.pyplot as plt
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    pipeline
)

print("Loading models... Please wait...\n")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==========================================
# LOAD MODELS
# ==========================================

# Chat model
chat_model_name = "microsoft/DialoGPT-medium"
chat_tokenizer = AutoTokenizer.from_pretrained(chat_model_name)
chat_model = AutoModelForCausalLM.from_pretrained(chat_model_name).to(device)

# Sentiment model
sentiment_analyzer = pipeline(
    "sentiment-analysis",
    device=0 if torch.cuda.is_available() else -1
)

# Summarizer (manual to avoid pipeline bugs)
summarizer_tokenizer = AutoTokenizer.from_pretrained("facebook/bart-large-cnn")
summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(
    "facebook/bart-large-cnn"
).to(device)

# QA model
qa_pipeline = pipeline(
    "question-answering",
    device=0 if torch.cuda.is_available() else -1
)

print("All models loaded successfully!\n")

# ==========================================
# PERSONALITY
# ==========================================

personalities = {
    "1": "Friendly and helpful assistant.",
    "2": "Professional and formal assistant.",
    "3": "Motivational and inspiring assistant."
}

print("Choose Personality:")
print("1 - Friendly")
print("2 - Professional")
print("3 - Motivational")

choice = input("Enter choice: ")
personality = personalities.get(choice, personalities["1"])

print(f"\nPersonality set to: {personality}\n")

# ==========================================
# VARIABLES
# ==========================================

temperature = 0.7
chat_history_ids = None
chat_log = []
sentiment_scores = []
long_term_memory = ""

# ==========================================
# FUNCTIONS
# ==========================================

def adapt_tone(sentiment_label):
    if sentiment_label == "NEGATIVE":
        return "Respond in a supportive and calming tone."
    elif sentiment_label == "POSITIVE":
        return "Respond in an enthusiastic tone."
    else:
        return "Respond professionally."

def summarize_conversation(text):
    if len(text.split()) < 50:
        return text

    inputs = summarizer_tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=1024
    ).to(device)

    summary_ids = summarizer_model.generate(
        inputs["input_ids"],
        max_length=100,
        min_length=30,
        num_beams=4,
        early_stopping=True
    )

    return summarizer_tokenizer.decode(
        summary_ids[0],
        skip_special_tokens=True
    )

def show_stats():
    positives = sentiment_scores.count("POSITIVE")
    negatives = sentiment_scores.count("NEGATIVE")

    plt.figure()
    plt.bar(["Positive", "Negative"], [positives, negatives])
    plt.title("Conversation Sentiment Distribution")
    plt.xlabel("Sentiment")
    plt.ylabel("Count")
    plt.show()

# ==========================================
# CHAT LOOP
# ==========================================

print("ðŸ¤– Advanced AI Assistant Started!")
print("Commands:")
print("/summary   â†’ Show conversation summary")
print("/stats     â†’ Show sentiment analytics")
print("/knowledge â†’ Ask factual question")
print("/exit      â†’ Exit\n")

mode = "chat"

while True:

    user_input = input("You: ")

    if user_input == "/exit":
        break

    if user_input == "/summary":
        combined_text = " ".join([entry["user"] for entry in chat_log])
        summary = summarize_conversation(combined_text)
        print("\nðŸ“„ Conversation Summary:\n", summary, "\n")
        continue

    if user_input == "/stats":
        show_stats()
        continue

    if user_input == "/knowledge":
        mode = "knowledge"
        print("Knowledge mode activated. Ask your question.\n")
        continue

    # ======================================
    # KNOWLEDGE MODE
    # ======================================
    if mode == "knowledge":
        context = long_term_memory if long_term_memory else "General knowledge context."
        result = qa_pipeline(question=user_input, context=context)
        print("Bot:", result["answer"], "\n")
        mode = "chat"
        continue

    # ======================================
    # SENTIMENT ANALYSIS
    # ======================================
    sentiment = sentiment_analyzer(user_input)[0]
    sentiment_label = sentiment["label"]
    sentiment_scores.append(sentiment_label)

    tone_instruction = adapt_tone(sentiment_label)

    enhanced_input = (
        f"{personality}\n"
        f"{tone_instruction}\n"
        f"Memory: {long_term_memory}\n"
        f"User: {user_input}"
    )

    new_input_ids = chat_tokenizer.encode(
        enhanced_input + chat_tokenizer.eos_token,
        return_tensors='pt'
    ).to(device)

    bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) \
        if chat_history_ids is not None else new_input_ids

    chat_history_ids = chat_model.generate(
        bot_input_ids,
        max_length=1000,
        pad_token_id=chat_tokenizer.eos_token_id,
        do_sample=True,
        temperature=temperature,
        top_p=0.9
    )

    response = chat_tokenizer.decode(
        chat_history_ids[:, bot_input_ids.shape[-1]:][0],
        skip_special_tokens=True
    )

    print("Bot:", response)
    print(f"(Detected Sentiment: {sentiment_label})\n")

    chat_log.append({
        "timestamp": str(datetime.datetime.now()),
        "user": user_input,
        "bot": response,
        "sentiment": sentiment_label
    })

    # Update long-term memory every 5 messages
    if len(chat_log) % 5 == 0:
        combined_text = " ".join([entry["user"] for entry in chat_log])
        long_term_memory = summarize_conversation(combined_text)
        print("ðŸ§  Long-term memory updated.\n")

# ==========================================
# SAVE CHAT
# ==========================================

save = input("Save chat history? (yes/no): ")

if save.lower() == "yes":
    with open("advanced_chat_history.json", "w") as f:
        json.dump(chat_log, f, indent=4)
    print("Chat saved successfully.")

